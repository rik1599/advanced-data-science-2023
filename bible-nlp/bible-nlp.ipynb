{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bible NLP\n",
    "\n",
    "In this notebook, we will apply some natural language processing techniques to some chocen texts from the Gospels.\n",
    "\n",
    "The goal is to verify the behavior of these techniques with these particular texts, in particular if they can capture information without changing the meaning of the text.\n",
    "\n",
    "The verses are taken using the API from [bible-api.com](https://bible-api.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Given a reference return the text of the verse\n",
    "def get_verse_text(verse: str):\n",
    "    query = verse.replace(' ', '+')\n",
    "    url = f'https://bible-api.com/{query}'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json = response.json()\n",
    "        return json['reference'], json['text'], response.status_code\n",
    "    else:\n",
    "        return verse, None, response.status_code\n",
    "\n",
    "def load_data(verses):\n",
    "    verses_text = {}\n",
    "    i = 0\n",
    "    for verse in verses:\n",
    "        wait = 4\n",
    "        while True:\n",
    "            ref, text, code = get_verse_text(verse)\n",
    "\n",
    "            if code == 200:\n",
    "                i += 1\n",
    "                verses_text[ref] = text\n",
    "                print(f'{i}/{len(verses)}: {ref}')\n",
    "                break\n",
    "            elif code == 429:\n",
    "                print(f\"Error {code}, next attempt in\", wait, \"seconds\")\n",
    "                time.sleep(wait)\n",
    "                wait *= 2\n",
    "            else:\n",
    "                print(f\"Error {code} for {ref}\")\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame({'verse': verses_text.keys(), 'text': verses_text.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_dataset(path_to_save: str, path_to_dataset: str):\n",
    "    verses = set()\n",
    "    for filename in os.listdir(path_to_dataset):\n",
    "        f = os.path.join(path_to_dataset, filename)\n",
    "        df = pd.read_csv(f, sep=',', encoding='utf-8', index_col=0)\n",
    "        for _, row in df.iterrows():\n",
    "            for col in df.columns:\n",
    "                if pd.notnull(row[col]):\n",
    "                    vs = row[col].split('\\n ')\n",
    "                    for verse in vs:\n",
    "                        verse = re.sub(r'[a-z]+', '', verse)\n",
    "                        verses.add(f\"{col} {verse}\")\n",
    "\n",
    "    df = load_data(verses)\n",
    "    df.to_csv(path_to_save, sep=',', encoding='utf-8')\n",
    "\n",
    "def load_categories(path_to_save: str, path_to_dataset: str):\n",
    "    verses = defaultdict(set)\n",
    "    for filename in os.listdir(path_to_dataset):\n",
    "        f = os.path.join(path_to_dataset, filename)\n",
    "        df = pd.read_csv(f, sep=',', encoding='utf-8', index_col=0)\n",
    "        for _, row in df.iterrows():\n",
    "            for col in df.columns:\n",
    "                if pd.notnull(row[col]):\n",
    "                    vs = row[col].split('\\n ')\n",
    "                    for verse in vs:\n",
    "                        verse = re.sub(r'[a-z]+', '', verse)\n",
    "                        verses[col].add(verse)\n",
    "    df.to_csv(path_to_save, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "We will start by classifying the texts into four categories:\n",
    "\n",
    "1. Events from the life of Jesus\n",
    "2. Parables\n",
    "3. Miracles\n",
    "4. Teachings or discourses of Jesus\n",
    "\n",
    "We have already classified the texts manually, the goal is to see and compare classification made by some different models (Decision Tree, Random Forest and Naive Bayes) and the real classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def preprocess(text, n):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    ngrams_set = set()\n",
    "    for i in range(1, n+1):\n",
    "        processed_text = ngrams(tokens, i)\n",
    "        ngrams_set.update([' '.join(gram) for gram in processed_text])\n",
    "    return ngrams_set\n",
    "\n",
    "def build_token_dictionary(tokens: pd.Series):\n",
    "    tokens_dict = defaultdict(set)\n",
    "    for idx, record in enumerate(tokens):\n",
    "        for token in record:\n",
    "            tokens_dict[token].add(idx)\n",
    "    return tokens_dict\n",
    "    \n",
    "def entropy(df: pd.DataFrame):\n",
    "    labels = set(df['category'].values)\n",
    "    entropy = 0\n",
    "    for label in labels:\n",
    "        p = len(df[df['category'] == label]) / len(df)\n",
    "        entropy -= p * np.log(p)\n",
    "    return entropy\n",
    "    \n",
    "def information_gain(df: pd.DataFrame, H, token: str):\n",
    "    token_df = df[df['tokens'].apply(lambda x: token in x)]\n",
    "    token_count = len(token_df)\n",
    "    token_entropy = entropy(token_df)\n",
    "    return H - token_count / len(df) * token_entropy\n",
    "    \n",
    "def to_numeric(df: pd.DataFrame, best_features: pd.DataFrame, threshold):\n",
    "    index = {}\n",
    "    for idx, token in enumerate(best_features['token']):\n",
    "        index[token] = idx\n",
    "    data = np.zeros((len(df), threshold))\n",
    "    target = np.zeros(len(df))\n",
    "    \n",
    "    for idx, record in df.iterrows():\n",
    "        for token in record['new_text']:\n",
    "            data[idx, index[token]] = 1\n",
    "        target[idx] = record['category']\n",
    "    return data, target\n",
    "\n",
    "def classification(clf, df: pd.DataFrame, verses_df: pd.Series, threshold):\n",
    "    df_tokens = df[:threshold]\n",
    "\n",
    "    new_texts = []\n",
    "    for tokens in verses_df['tokens']:\n",
    "        intesect = set(tokens).intersection(set(df_tokens['token'].values))\n",
    "        new_texts.append(intesect)\n",
    "    verses_df['new_text'] = new_texts\n",
    "    numeric_df, target = to_numeric(verses_df, df_tokens, threshold)\n",
    "    \n",
    "    # The classifier is trained with a 10-fold cross validation\n",
    "    scores = cross_val_score(clf, numeric_df, target, cv=10)\n",
    "    return scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare mean and standard deviations of the cross-validation scores of the three models varying the number of features used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.exists('data/verses.csv'):\n",
    "    load_dataset('data/verses.csv', 'data/verses')\n",
    "\n",
    "if not os.path.exists('data/categories.csv'):\n",
    "    load_categories('data/categories.csv', 'data/verses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses_df['tokens'] = verses_df['text'].apply(preprocess, n=3)\n",
    "\n",
    "tokens_dict = build_token_dictionary(verses_df['tokens'])\n",
    "df_tokens = pd.DataFrame(zip(tokens_dict.keys(), tokens_dict.values()), columns=['token', 'verses'])\n",
    "\n",
    "count = []\n",
    "for t in df_tokens['verses']:\n",
    "    count.append(len(t))\n",
    "df_tokens['count'] = count\n",
    "df_tokens = df_tokens[df_tokens['count'] > 1]\n",
    "H = entropy(verses_df)\n",
    "df_tokens['information_gain'] = df_tokens['token'].apply(lambda x: information_gain(verses_df, H, x))\n",
    "df_tokens.sort_values(by='information_gain', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'childhood'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m x_axis \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m, \u001b[39m450\u001b[39m, \u001b[39m50\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x_axis:\n\u001b[0;32m---> 14\u001b[0m     mean_dt, std_dt \u001b[39m=\u001b[39m classification(decision_tree, df_tokens, verses_df, i)\n\u001b[1;32m     15\u001b[0m     mean_rf, std_rf \u001b[39m=\u001b[39m classification(random_forest, df_tokens, verses_df, i)\n\u001b[1;32m     16\u001b[0m     mean_nb, std_nb \u001b[39m=\u001b[39m classification(naive_bayes, df_tokens, verses_df, i)\n",
      "Cell \u001b[0;32mIn[6], line 64\u001b[0m, in \u001b[0;36mclassification\u001b[0;34m(clf, df, verses_df, threshold)\u001b[0m\n\u001b[1;32m     62\u001b[0m     new_texts\u001b[39m.\u001b[39mappend(intesect)\n\u001b[1;32m     63\u001b[0m verses_df[\u001b[39m'\u001b[39m\u001b[39mnew_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m new_texts\n\u001b[0;32m---> 64\u001b[0m numeric_df, target \u001b[39m=\u001b[39m to_numeric(verses_df, df_tokens, threshold)\n\u001b[1;32m     66\u001b[0m \u001b[39m# The classifier is trained with a 10-fold cross validation\u001b[39;00m\n\u001b[1;32m     67\u001b[0m scores \u001b[39m=\u001b[39m cross_val_score(clf, numeric_df, target, cv\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 53\u001b[0m, in \u001b[0;36mto_numeric\u001b[0;34m(df, best_features, threshold)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m record[\u001b[39m'\u001b[39m\u001b[39mnew_text\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     52\u001b[0m         data[idx, index[token]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 53\u001b[0m     target[idx] \u001b[39m=\u001b[39m record[\u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     54\u001b[0m \u001b[39mreturn\u001b[39;00m data, target\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'childhood'"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mean = []\n",
    "std = []\n",
    "decision_tree = DecisionTreeClassifier(random_state=0)\n",
    "random_forest = RandomForestClassifier(random_state=0)\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "x_axis = range(100, 450, 50)\n",
    "\n",
    "for i in x_axis:\n",
    "    mean_dt, std_dt = classification(decision_tree, df_tokens, verses_df, i)\n",
    "    mean_rf, std_rf = classification(random_forest, df_tokens, verses_df, i)\n",
    "    mean_nb, std_nb = classification(naive_bayes, df_tokens, verses_df, i)\n",
    "    mean.append((mean_dt, mean_rf, mean_nb))\n",
    "    std.append((std_dt, std_rf, std_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "labels = ['Decision Tree', 'Random Forest', 'Naive Bayes']\n",
    "\n",
    "ax1.plot(x_axis, mean, label=labels)\n",
    "ax1.set(ylabel='Mean accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(x_axis, std, label=labels)\n",
    "ax2.set(xlabel='Number of features', ylabel='Standard deviation')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison has not indicated a clear winner, mainly because the scores change a lot during every run of the notebook. \n",
    "In general:\n",
    "\n",
    "- the best model seems to be the Multinomial Naive Bayes, but the difference is not significant,\n",
    "\n",
    "- the accuracy of the models is almost always upper than 70%, that is not bad, but the same consideration made before applies here too: the scores change a lot during every run of the notebook (even if the random seed is fixed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "In this section we will try to cluster the texts using K-Means and TD-IDF technique.\n",
    "\n",
    "The goal is to see if parallel texts are clustered together varying the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARALLELS = [\n",
    "    {'Matthew 1:18-25', 'Luke 2:1-7'},                                      # Birth of Jesus\n",
    "    {'Matthew 3:13-17', 'Mark 1:9-11', 'Luke 3:21-22', 'John 1:29-34'},     # Baptism of Jesus\n",
    "    {'Matthew 4:1-11', 'Mark 1:12-13', 'Luke 4:1-13'},                      # Temptation of Jesus\n",
    "    {'Matthew 21:12-13', 'Mark 11:15-18', 'Luke 19:45-46', 'John 2:14-21'}, # Cleansing of the Temple\n",
    "    {'Matthew 8:14-15', 'Mark 1:29-31', 'Luke 4:38-39'},                    # The Healing of Peter's Mother-in-law\n",
    "    {'Matthew 10:1-4', 'Mark 3:13-19', 'Luke 6:12-16'},                     # The Twelve Apostles\n",
    "    {'Matthew 6:7-15', 'Luke 11:1-4'},                                      # The Lord's Prayer\n",
    "    {'Matthew 5:43-48', 'Luke 6:27-36'},                                    # Love for Enemies\n",
    "    {'Matthew 9:18-26', 'Mark 5:21-43', 'Luke 8:40-56'},                    # Jesus heals Giarius' daughter\n",
    "    {'Matthew 8:28-34', 'Mark 5:1-20', 'Luke 8:26-39'},                     # Jesus heals the Gerasene demoniac\n",
    "    {'Matthew 13:1-9', 'Mark 4:1-9', 'Luke 8:4-8'},                         # Parable of the Sower\n",
    "    {'Matthew 13:18-23', 'Mark 4:13-20', 'Luke 8:11-15'},                   # Parable of the Sower explained\n",
    "    {'Matthew 14:13-21', 'Mark 6:32-44', 'Luke 9:10-17', 'John 6:1-15'},    # Jesus feeds the five thousand\n",
    "    {'Matthew 17:1-8', 'Mark 9:2-8', 'Luke 9:28-36'},                       # Transfiguration of Jesus\n",
    "    {'Matthew 21:12-13', 'Mark 11:15-18', 'Luke 19:45-46', 'John 2:14-21'}, # Cleansing of the Temple\n",
    "    {'Matthew 27:55-56', 'Mark 15,40-41', 'Luke 23:49', 'John 19:25-27'},   # Witnesses to the crucifixion\n",
    "    {'Matthew 28:1-10', 'Mark 16:1-8', 'Luke 24:1-12', 'John 20:1-18'},     # Resurrection of Jesus\n",
    "]\n",
    "\n",
    "def clustering_verses():\n",
    "    verses = set()\n",
    "    for p in PARALLELS:\n",
    "        verses |= p\n",
    "\n",
    "    verses_df = load_data(verses)\n",
    "    verses_df.to_csv('data/clustering_verses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def preprocess(text, n):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [token for token in tokens if token.isalpha() and (\n",
    "        token not in stopwords.words('english'))]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(\n",
    "        token) for token in filtered_tokens]\n",
    "    if n <= 1:\n",
    "        return lemmatized_tokens\n",
    "    ngram_set = []\n",
    "    for i in range(1, n + 1):\n",
    "        processed_text = ngrams(lemmatized_tokens, i)\n",
    "        ngram_set.extend([' '.join(grams) for grams in processed_text])\n",
    "    return ngram_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.exists('data/clustering_verses.csv'):\n",
    "    clustering_verses()\n",
    "\n",
    "verses_df = pd.read_csv('data/clustering_verses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses_df['tokens'] = verses_df['text'].apply(preprocess, n=1)\n",
    "verses_df['tokens'] = verses_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.95)\n",
    "X = vectorizer.fit_transform(verses_df['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As metric for the accuracy of the clustering we will measure for each verses how many verses from the same set of parallel verses are in the same cluster and calculate the mean of this values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(df, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=1234567, n_init='auto').fit(X)\n",
    "    clusters = kmeans.labels_\n",
    "    df['cluster'] = clusters\n",
    "\n",
    "    guessed_parallels = []\n",
    "    fractions = []\n",
    "    for v in verses_df['verse']:\n",
    "        v_set = None\n",
    "        v_cluster = df[df['verse'] == v]['cluster'].values\n",
    "        for p in PARALLELS:\n",
    "            if v in p:\n",
    "                v_set = p\n",
    "                break\n",
    "    \n",
    "        i = 0\n",
    "        for v_p in v_set:\n",
    "            v_p_cluster = df[df['verse'] == v_p]['cluster'].values\n",
    "            if v != v_p and v_cluster == v_p_cluster:\n",
    "                i += 1\n",
    "        guessed_parallels.append(i / (len(v_set)-1))\n",
    "        fractions.append(f\"{i} / {len(v_set)-1}\")\n",
    "    return sum(guessed_parallels) / len(guessed_parallels), fractions, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis = range(2, len(PARALLELS))\n",
    "y_axis = []\n",
    "\n",
    "best_accuracy = 0\n",
    "best_clusters = None\n",
    "best_n_clusters = 0\n",
    "best_fractions = None\n",
    "for i in x_axis:\n",
    "    accuracy, fractions, clusters = clustering(verses_df, i)\n",
    "    if accuracy >= best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_clusters = clusters\n",
    "        best_n_clusters = i\n",
    "        best_fractions = fractions\n",
    "    y_axis.append(accuracy)\n",
    "\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "pca = PCA(n_components=2, random_state=1234567)\n",
    "pca_vecs = pca.fit_transform(X.toarray())\n",
    "x = pca_vecs[:, 0]\n",
    "y = pca_vecs[:, 1]\n",
    "verses_df['cluster'] = best_clusters\n",
    "verses_df['guessed parallels'] = best_fractions\n",
    "verses_df['x'] = x\n",
    "verses_df['y'] = y\n",
    "\n",
    "plt.title(f\"TF-IDF Clusters (n° clusters={best_n_clusters})\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "sns.scatterplot(data=verses_df, x=\"x\", y=\"y\", hue=\"cluster\", palette=\"deep\", legend=False)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Percentage of guessed parallels: {best_accuracy*100:.2f}%\")\n",
    "verses_df.drop(columns=['tokens', 'x', 'y']).sort_values(by= ['cluster', 'verse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very good: we obtained a 96% accuracy from 13 to 15 clusters, which is very similar to the real ones, but in general we observe that the accuracy is always upper than 70% for every number of clusters, which is not bad.\n",
    "\n",
    "Badly the results are influenced by the random seed used, so for making more accurate considerations we should try different random seeds.\n",
    "\n",
    "Another possible approach for findings parallel verses is to use a hierarchical clustering algorithm (where number of clusters are not known a priori), but we have not tried it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "In this section we will try to summarize some texts using Sentence Scoring and Sentence Extraction techniques.\n",
    "\n",
    "- The Sermon on the Mount (Matthew 5-7)\n",
    "\n",
    "- The Last Supper from the Gospel of John (John 13-17)\n",
    "\n",
    "- The Parable of the Prodigal Son (Luke 15,11-32)\n",
    "\n",
    "- The Passion of Jesus from the Gospel of Luke (Luke 22-23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizing_verses():\n",
    "    verses = [\n",
    "        'Matthew 5:1-7:29', # Sermon on the Mount\n",
    "        'John 13:1-17:26',  # The Last Supper\n",
    "        'Luke 22:1-23:56',  # The Passion\n",
    "        'Luke 15:11-32',    # Parable of the Prodigal Son\n",
    "    ]\n",
    "    verses_df = load_data(verses)\n",
    "    verses_df.to_csv('data/summarizing_verses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.exists('data/summarizing_verses.csv'):\n",
    "    summarizing_verses()\n",
    "    \n",
    "verses_df = pd.read_csv('data/summarizing_verses.csv')\n",
    "verses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def preprocess(text, lemmatizer, n):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha(\n",
    "    ) and token not in stopwords.words('english')]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    if n <= 1:\n",
    "        return tokens\n",
    "\n",
    "    ngrams_set = []\n",
    "    for i in range(1, n+1):\n",
    "        processed_text = ngrams(tokens, i)\n",
    "        ngrams_set.extend([' '.join(grams) for grams in processed_text])\n",
    "    return ngrams_set\n",
    "\n",
    "def gen_summary(sentences, scores, threshold):\n",
    "    summary = []\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        s = re.sub(r'\\s+', ' ', sentence)\n",
    "        if scores[idx] >= threshold:\n",
    "            summary.append(s)\n",
    "    return summary\n",
    "\n",
    "def threshold(*args):\n",
    "    return args[1] * np.mean(list(args[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Sentence Scoring, instead of using frequentistic methods (like TF-IDF), we will use Latent Dirichlet Allocation (LDA) to find the most important topics in the text and use the probability of the tokens in the topics as weight. If a tokens compares in more than one topic, the sum of the probabilities is used.\n",
    "\n",
    "The score of a sentence is calculated in this way:\n",
    "\n",
    "1. the sentence is tokenized in words and the tokens are lemmatized\n",
    "\n",
    "2. for each token if the token is in the list of the most important tokens obtained from LDA, the score of the token is added to the score of the sentence\n",
    "\n",
    "3. the score of the sentence is divided by the number of relevant tokens found during the previous step (in this way short sentences are not penalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "def text_lda(sentences):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [preprocess(sentence, lemmatizer, 1) for sentence in sentences]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    lda = models.LdaModel(corpus, id2word=dictionary, num_topics=4)\n",
    "\n",
    "    scores = defaultdict(lambda: 0)\n",
    "    for topic, _ in lda.top_topics(corpus, coherence='u_mass', topn=10):\n",
    "        for score, token in topic:\n",
    "            scores[token] += score\n",
    "\n",
    "    return dict(scores)\n",
    "\n",
    "def score_sentence_lda(sentences, lda_scores, lemmatizer):\n",
    "    sentence_score = {}\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        score = 0\n",
    "        relevant_tokens = 0\n",
    "        for token in word_tokenize(sentence.lower()):\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            if token in lda_scores:\n",
    "                score += lda_scores[token]\n",
    "                relevant_tokens += 1\n",
    "        sentence_score[idx] = score / relevant_tokens if relevant_tokens > 0 else 0\n",
    "    return sentence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "verses_df['sentences'] = verses_df['text'].apply(sent_tokenize)\n",
    "verses_df['lda_scores'] = verses_df['sentences'].apply(text_lda)\n",
    "verses_df['sentence_scores'] = verses_df.apply(lambda x: score_sentence_lda(x['sentences'], x['lda_scores'], lemmatizer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses_df['alpha'] = [1.8, 1.5, 1.6, 1]\n",
    "verses_df['summary'] = verses_df.apply(lambda x: gen_summary(x['sentences'], x['sentence_scores'], threshold(x['sentence_scores'].values(), x['alpha'])), axis=1)\n",
    "\n",
    "for _, row in verses_df.iterrows():\n",
    "    print(row['verse'])\n",
    "    print('\\n'.join(row['summary']))\n",
    "    print(f\"\\nOriginal size: {len(row['sentences'])} sentences, summary size: {len(row['summary'])} sentences\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used for Sentence Scoring seems to work well.\n",
    "\n",
    "The summaries obtained are coherent with the original texts. In particular we observe a tendency to summarize the texts using long sentences, which is good for this kind of texts, but it is bad for the size of the summaries.\n",
    "\n",
    "Very appreciated is the fact that the start of dialogs are almost always included in the summaries, which is very important for understanding the context (this was a problem with the summaries obtained using frequentistic methods like TF-IDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The results obtained for the three tasks are very good, but they suffer from the non-deterministic behavior of the models used.\n",
    "\n",
    "The classification task is the one that suffers more from this problem, but the results are still good, so we can conclude that the models used are good for this kind of texts.\n",
    "\n",
    "Instead, the results obtained for clustering task are very good, but I think that the results are biased by the fact we know the real number of clusters, so we can't conclude that K-means is not a good algorithm for finding parallel verses, but algorithm with a non-deterministic number of clusters (like hierarchical clustering) should be tried. K-means could be used for verifying the results obtained with other algorithms.\n",
    "\n",
    "For the summarization task, the result are promising, so I think that the model used is good for this kind of texts, but the summaries obtained quite long, so the model should be improved, for example, trying different methods for scoring the sentences and calculating the summaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
